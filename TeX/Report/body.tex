
check for ``solving frictional contact problem''
\begin{itemize}
\item  in google, and scholar
\item Zentralblatt and MAthSciNet
\end{itemize}
        


\section{Introduction}

\subsection{Objectives}
\begin{itemize}
\item Simple iteratives  methods
  \begin{itemize}
  \item Fixed point 
  \item Projection/splitting (PSOR)
  \item Fake Coulomb Friction (Anitescu \& Tasora)
  \end{itemize}
\item zeroes of functions
  \begin{itemize}
  \item Alart-Curnier method
  \item CKPS method
  \item DeSaxce + Newton (Joli Feng)
  \item Newton + line search (GP, Armijo, Non-monotone watch dogs)
  \end{itemize}
\item Optimisation-based methods
  \begin{itemize}
  \item ACLM (Fixed point, Newton, Quasi-Newton, ....)
  \item SOCLCP (Kanno, et al.)
  \item Successive approximations (Haslinger, ...) QP et SOC (Kucera)
  \end{itemize}
\end{itemize}

En options
\begin{itemize}
\item SOCCP
\item Interior point
\item NCP (Fischer--bursmeister for SOC) smoothing hayashima fukushima ...
\end{itemize}

\subsection{Test}

\begin{itemize}
\item collections
  \begin{itemize}
  \item spheres 
    \begin{itemize}
    \item flows and stacking (Example Tasora)
    \end{itemize}
  \item sticks
    \begin{itemize}
    \item flows and stacking (Example Tasora)
    \end{itemize}
  \item hair, LMGC clumps ??
  \end{itemize}
\item deformables quasi-static / dynamic
  \begin{itemize}
  \item Hertz 3D FEM
  \item ...
  \end{itemize}
\end{itemize}

\subsection{parameters}

\begin{itemize}
\item size (n, m), sparsity,
\item matrix storage
\item conditionning $M$ $W$
\item rank of $H$ or $W$ on active contact
\item parameter $\nu = m_c/n$
\end{itemize}


\begin{itemize}
\item algorithms parameters ($\rho$)
\end{itemize}

\subsection{parameters}

\begin{itemize}
\item tolerance
\item CPU time and memory or better flops (papi)
\item Convergence rate (error w.r.t. flop)
\item Scability
\item Max time. 
\item Obtained accuracy
\item 
\end{itemize}


\subsection{What is not in this paper}

We restrict ourselves to 3D friction with a nonlinear friction cone:
\begin{itemize}
\item no LCP
\item no pivoting theory
\end{itemize}
\clearpage
\section{Description of the 3D frictional contact problems}

\subsection{Signorini's condition and Coulomb's friction.}

Let us consider first the contact between two  bodies $A \subset  \R^3$ and $B \subset \R^3$ with sufficiently smooth boundaries. (see Figure \ref{fig:local-frame}). Let us consider a point $C_{A} \in \partial A$ called a master point to contact.  The choice of this master point $C_A$ to write the contact condition is crucial in  practice and amounts to consistently discretizing  the contact surface. The vector  ${\sf N}$ defines an outward unit normal vector to $A$ at the point $C_A$. The normal vector is completed with two tangent vectors $\sf T_1, T_2$ such that $(C_A, \sf N, T_1, T_2)$ is an orthornormal frame called the \textit{local frame at contact}. The slave contact point $C_B \in \partial B$ is defined as the projection of the point $C_A$ on $\partial B$ in the direction given by $\sf N$. Note that we assume that such a point exists. The gap function is defined as the signed distance between $C_A$ and $C_B$
\begin{equation}
  \label{eq:gap}
  g_{\n} = (C_B-C_A)  \sf N.
\end{equation}


To fix ideas, if we consider two strictly convex bodies, which are non penetrating, {\it i.e.}  $A \cap B = \emptyset$, the master and slave contact points can be chosen as the proximal points of each bodies and the normal vector  ${\sf N}$ can be written as
\begin{equation}
  \label{eq:normal}
  {\sf N} = \Frac{C_B-C_A}{\|C_B-C_A\|}.
\end{equation}



The contact force exerted by $A$ on $B$ is denoted by $r \in \RR^3$ and is decomposed in the local frame as
\begin{equation}
  \label{eq:reaction}
  r =  r_\n {\sf N} + r_\t, \quad \text{ with  } r_\n \in \RR \text{ and } r_\t \in \RR^2.
\end{equation}

The \textit{Signorini condition} states that
\begin{equation}
  \label{eq:signo}
  0 \leq g_{\n} \perp r_\n \geq 0.
\end{equation}
 and models the unilateral contact. The condition~(\ref{eq:signo}) which is written at the position level can also be defined at the velocity level. To that aim, the relative velocity $u \in \RR^3$ of the point $C_{B}$ with respect to $C_{A}$ is also decomposed in the local frame
\begin{equation}
  \label{eq:velocity}
  u =  u_\n {\sf N} + u_\t, \quad \text{ with } u_\n \in \RR \text{ and } u_\t \in \RR^2.
\end{equation}
The velocity level formulation of the Signorini condition is written 
\begin{equation}
  \label{eq:signo-velocity}
  \left\{\begin{array}{ll}
  0 \leq u_{\n} \perp r_\n \geq 0  &\text{ if } g_{\n} \leq 0 \\
  r_{\n} =0 &\text{ otherwise}.
\end{array}\right.
\end{equation}
The Moreau's viability Lemma~\cite{Moreau1988} ensures that~(\ref{eq:signo-velocity}) implies (\ref{eq:signo}) if $g_{\n}\geq 0$ holds in the initial configuration.

Coulomb's friction  models the frictional behavior of the contact force law in the tangent plane. Let us define the Coulomb friction  cone $K$ which is chosen as the isotropic second order cone 
\begin{equation}
  \label{eq:CoulombCone}
  K = \{r \in \RR^3 \mid \|r_\t\| \leq \mu r_n\},
\end{equation}
where $\mu$ is the coefficient of friction. The Coulomb friction  states for the sticking case that 
\begin{equation}
  \label{eq:Coulom-stick}
  u_{\t} =0,\quad r \in K
\end{equation}
and for the sliding case that
\begin{equation}
  \label{eq:Coulom-slide}
  u_{\t}  \neq 0,\quad r \in \partial K, \exists\, \alpha > 0, r_\t = -\alpha u_\t.
\end{equation}

\paragraph{Disjunctive formulation of the Signorini-Coulomb model}

If we consider the velocity-level Signorini condition~(\ref{eq:signo-velocity}) together with the Coulomb friction~(\ref{eq:Coulom-stick})--(\ref{eq:Coulom-slide}) which is naturally expressed in terms of velocity, we obtain a disjunctive formulation of the frictional contact behavior as
\begin{equation}
  \label{eq:contact-disjunctive}
  \left\{\begin{array}{llr}
      r = 0  &\text{ if } g_{\n} > 0  & \text{(no contact)}\\
      r = 0,  u_\n \geq 0   &\text{ if } g_{\n} \leq 0 & \text{(take--off)} \\
      r \in K, u =0 &\text{ if } g_{\n} \leq 0 & \text{(sticking)}  \\
      r \in \partial K,u _\n=0,  \exists\,\alpha > 0, u_\t = -\alpha r_\t &\text{ if } g_{\n} \leq 0 & \text{(sliding)}  \\
\end{array}\right.
\end{equation}
In the computational practice, the disjunctive formulation is not suitable for  solving the Coulomb problem as it suggests the use of enumerative solvers. In the sequel, alternative formulations of the Signorini-Coulomb model prone to numerical applications will be delineated.

\paragraph{Inclusion into normal cones} The Signorini condition~(\ref{eq:signo}) in their complementarity forms can be equivalently written  as an inclusion into a normal cone to $\RR_+$
\begin{equation}
  \label{eq:signo-inclusion}
  - g_\n \in N_{\RR_+}(r_\n),
\end{equation}
and respectively for~(\ref{eq:signo-velocity})
\begin{equation}
  \label{eq:signo-inclusion-velo}
  - u_\n \in N_{\RR_+}(r_\n),
\end{equation}
if $g_{\n} \leq 0$ and $r_\n =0 $ otherwise. An inclusion form of the Coulomb friction can be also proposed introducing the Coulomb disk $D(\mu r_n)$
\begin{equation}
  \label{eq:diskR}
  D(\mu r_\n) = \{r_\t \in \RR^2\mid \|r_T\| \leq \mu r_\n   \}.
\end{equation}
For the Coulomb friction, we get
\begin{equation}
  \label{eq:Coulomb-inclusion}
  - u_{\t} \in N_{D(\mu r_\n)}.
\end{equation}
Note that (\ref{eq:Coulomb-inclusion}) is not equivalent to a complementarity problem since $D(\mu r_\n)$ is not a cone.  The formualation~(\ref{eq:Coulomb-inclusion}) is often related to Moreau's maximum dissipation principle of the frictional behavior,
\begin{equation}
  \label{eq:moreau-maxprinciple}
  r_\t = \mbox{argmax}_{\|z\| \leq \mu r_n}  z^\top u_\t.
\end{equation}


\paragraph{SOCCP formulation of the Signorini-Coulomb model}

In \cite{Acary.Brogliato2008, Acary.ea2010}, another formulation is proposed inspired by the so-called bipotential \cite{DeSaxce92}. Operating a change of variable, we introduce the modified relative velocity $\hat u \in \RR^3$ defined by
\begin{equation}
  \label{eq:modified-velocity}
  \hat u = u +\mu \|u_\t\| \sf N.
\end{equation}
The entire contact model (\ref{eq:contact-disjunctive}) can be put into a Second-Order Cone Complementarity Problem (SOCCP) as
\begin{equation}
  \label{eq:contact-SOCCP}
 K^\star \ni \hat u \perp r \in K
\end{equation}
 if $ g_\n \leq 0 $ and $r=0$ otherwise. The set $ K^\star $ is the dual convex cone to $K$ defined by
\begin{equation}
  \label{eq:dual-cone}
  K^\star = \{u \in \RR^3 \mid  r^\top u \geq 0, \quad \text{for all } r \in K   \}.
\end{equation}







\subsection{Frictional contact discrete problems}

In this section, we formulate two basic discrete frictional contact problems considering a finite number $n$ of degrees of freedom  together with a discrete linear dynamics. We assume that a finite set of $n_c$ contact points and their associated local frames has been defined. In the general, this task is not straightforward and amounts to correctly discretizing  the contact surfaces. For more details, we refer to \cite{Wriggers2006,Laursen2003}. Let us denote by the integer $m = 3 n_c$ is the number of unknown variables at contacts in a three-dimensional contact configuration.

For each contact $\alpha \in \{1,\ldots n_c\}$, the  local velocity  is denoted by $u^\alpha \in \RR^3$, the normal velocity by $u_\n^{\alpha}\in \RR$ and the tangential velocity by $u_\t^\alpha\in\RR^2$. One obviously 
gets
\begin{equation}
  \label{eq:contactvelocity}
  u^\alpha =\left[
  \begin{array}{c}
    u^\alpha_{\n} \\
    u^\alpha_{\t}   
  \end{array}\right]
\end{equation}
The vector $u, u_\n, u_\t$ respectively collects all the local velocity
$  u = [[u^\alpha]^\top, \alpha = 1\ldots n_c]^\top$,
all the normal velocity 
$
  u_\n = [ u^\alpha_{\n}, \alpha = 1\ldots n_c]^\top$,
and all the  tangential velocity
$
  u_\t = [ [u^\alpha_{\t}]^\top, \alpha = 1\ldots n_c]^\top$.
For a contact $\alpha $, the modified local velocity, denoted by $\hat u^\alpha $, is defined by
\begin{equation}
  \label{eq:modified}
  \hat u^\alpha = u^\alpha + g^\alpha(u)
\end{equation}
where
\begin{equation}
  \label{eq:modified-bis}
  g^\alpha(u) =  \mu^\alpha  \|u^\alpha_\t\| {\sf N}^\alpha.
\end{equation}
The vectors $\hat u$ and $g(u)$ collect all the modified local velocity at each contact $\hat u = [[\hat u^\alpha]^\top, \alpha = 1\ldots n_c]^\top$ and the function $g(u) = [[\mu^\alpha  \|u^\alpha_\t\| {\sf N}^\alpha]^\top, \alpha = 1\ldots n_c]^\top$. Let us introduce finally the following notation to express the function $g(u)$ in a convenient manner. We denoted by $s^\alpha = \|u^\alpha_\t\|$ the norm of the sliding velocity for the contact $\alpha$. The vector $s$ collects all the sliding velocity at each contact $s = [ s^\alpha, \alpha = 1\ldots n_c]^\top$. The matrix $ N \in \RR^{3n_c\times n_c}$ is composed of the vector $ mu^\alpha N^\alpha$ on its diagonal formally $N = \mbox{diag}(\mu^\alpha N^\alpha$. The function $g()$ can be therefore expressed as $g(u) = N s$.

For each contact $\alpha$, the reaction vector $r^\alpha\in \RR^3$ is also decomposed in its normal part $r_\n^{\alpha}\in \RR$ and the tangential part $r_\t^\alpha\in\RR^2$ as
\begin{equation}
  \label{eq:contactreaction}
  r^\alpha = \left[
  \begin{array}{c}
    r^\alpha_{\n} \\
    r^\alpha_{\t}   
  \end{array}\right]
\end{equation}
The Coulomb friction cone for a  contact $\alpha$ is defined by $K^{\alpha}  = \{r^\alpha, \|r^\alpha_\t \| \leq \mu^\alpha |r^\alpha_\n| \}$ and the set $K^{\alpha,\star}$ is its dual. 
The set $K$ is the cartesian product of Coulomb's friction cone at each contact, that 
\begin{equation}
  \label{eq:CC}
  K = \prod_{\alpha=1\ldots n_c} K^{\alpha} 
\end{equation}
and $K^\star$ is dual.

Let us now introduce two basic frictional contact problems.
\begin{problem}[General discrete frictional contact problem]\label{prob:I}
  Given
  \begin{itemize}
    \item a symmetric positive definite matrix ${M} \in \nbR^{n \times n}$,
    \item a vector $ {f} \in \nbR^n$,
    \item a matrix  ${H} \in \nbR^{n \times m}$,
    \item a vector $w \in \RR^{m}$,
    \item a vector of coefficients of friction $\mu \in \RR^{n_c}$,
  \end{itemize}
find three vectors $ {v} \in \nbR^n$, $u\in\RR^m$ and $r\in \RR^m$, denoted by $\mathrm{FC/I}(M,H,f,w,\mu)$  such that
\begin{equation}\label{eq:soccp1}
  \begin{cases}
    M v = {H} {r} + {f} \\[2mm]
    u = H^\top v + w \\[2mm]
    \hat u = u + g(u) \\[2mm]
    K^\star \ni {\hat u} \perp r \in K
  \end{cases}
\end{equation}
with $g(u) = [[\mu^\alpha  \|u^\alpha_\t\| {\sf N}^\alpha]^\top, \alpha = 1\ldots n_c]^\top$. 
\qed
\end{problem}

\begin{problem}[Reduced discrete frictional contact problem]\label{prob:II}
  Given
  \begin{itemize}
    \item a symmetric positive semi--definite  matrix ${W} \in \nbR^{m \times m}$,
    \item a vector $ {q} \in \nbR^m$,
    \item a vector $\mu \in \RR^{n_c}$ of coefficients of friction, 
  \end{itemize}
find two vectors $u\in\RR^m$ and $r\in \RR^m$, denoted by $\mathrm{FC/II}(W,q,\mu)$  such that
\begin{equation}\label{eq:soccp2}
  \begin{cases}
    u =Wr +q \\[2mm]
    \hat u =u + g(u) \\[2mm]
    K^\star \ni {\hat u} \perp r \in K
  \end{cases}
\end{equation}
with $g(u) = [[\mu^\alpha  \|u^\alpha_\t\| {\sf N}^\alpha]^\top, \alpha = 1\ldots n_c]^\top$.
\qed
\end{problem}



\paragraph{Comments} The variable $\hat u$ does not explicitly appear as a variable of the problem since the mapping $u \mapsto u+g(u)$ is a one--to--one mapping.

The  first two lines of (\ref{eq:soccp1}) may represent a linear time--discretized dynamics and $v$ have the role of the global or generalized velocity. We will see in the next section how these systems can be  obtained and how they may also be an instance of a quasi-static problem

 Nonlinear versions of this problem can also be stated but they are out pf the scope of the paper. When the dynamics is nonlinear, an outer Newton linearization is performed leading to a linearized problem. 


Problem~\ref{prob:II} is often referenced as the \textit{condensed} or \textit{reduced} problem. The two problems are closely related. Indeed, if we consider the inverse of the matrix $M$ we obtain an explicit equation for $v$ in Problem~\ref{prob:I}
\begin{equation}
  \label{eq:vv}
  v = M^{-1}\,(Hr +f).
\end{equation}
Substituting (\ref{eq:vv}) in the first line of~(\ref{eq:soccp1}), we get
\begin{equation}
  \label{eq:vv-1}
  u = H^\top M^{-1} H r + H^\top M^{-1} f +w.
\end{equation}
The matrix $W$, often called the \textit{Delassus matrix}, is easily identified as
\begin{equation}
  \label{eq:Delassus}
  W = H^\top M^{-1} H 
\end{equation}
and the vector $q$ as
\begin{equation}
  \label{eq:qq}
  q = H^\top M^{-1} f +w.
\end{equation}


 \begin{ndrva}
   one word in the existence criteria
 \end{ndrva}

\subsection{Origins of the discrete problem}

\begin{ndrva}
  Is it mandatory ?
\end{ndrva}


\subsubsection{Finite freedom mechanics}



\subsubsection{Quasi-static frictional contact problem}

\cite{Haslinger1983,Christensen.Klarbring.ea1998,Klarbring.Pang1999}

\subsubsection{Dynamic frictional contact problem}

\cite{Moreau1998,Moreau1994,Moreau1999}

\clearpage
%----------------------------------------------------------------------------------%
\section{Various formulations prone to  numerical methods}
%----------------------------------------------------------------------------------%
In this section, various equivalent formulations of Problems~\ref{prob:I} and~\ref{prob:II} are given. We aim at showing that such problems can be recast into several well-known problems of the mathematical programming and optimization community.

\subsection{Variational inequalities(VI) and Quasi-Variational inequalities(QVI)}

For the standard  theory of finite--dimensional variational inequalities, we refer to \cite{Harker.Pang1990} and \cite{Facchinei.Pang2003}. Let us define a finite-dimensional VI denoted by $\mathrm{VI}(X,F)$,
\begin{equation}
  \label{eq:vi}
  F^\top(z)(y-z) \geq 0, \text{ for all } y \in X,
\end{equation}
with $X$ be a nonempty subset of $\RR^n$ and let $F$ be a mapping from $\RR^n$ into itself. The easiest way to state equivalent VI formulations of Problems~\ref{prob:I} and~\ref{prob:II} is to notice that
\begin{equation}
  \label{eq:SOCCP-1}
  K^\star \ni {\hat u} \perp r \in K,
\end{equation}
is equivalent to the following inclusion into a normal cone
\begin{equation}
  \label{eq:SOCCP-2}
  - {\hat u} \in N_K(r),
\end{equation}
which is  in turns equivalent to the following VI
\begin{equation}
  \label{eq:VI}
 \hat u^\top (s -r) \geq 0, \text{ for all } s \in K.
\end{equation}

For Problem~\ref{prob:I}, let us start by rewriting the problem~(\ref{eq:soccp1}) in a convenient form
\begin{equation}
  \label{eq:soccp1-bis}
  \left\{\begin{array}{l}
    M v-H r-f=0, \\[2mm]
    u - H^\top v - w =0, \\[2mm]
   -(u + g(u)) \in N_K(r).
 \end{array}\right.
\end{equation}
 Let us  define by $\bar K = \RR^n \times \RR^m \times K\subset \RR^{2m+n}$. The problem (\ref{eq:soccp1-bis}) is written as an inclusion into the normal cone to $\bar K$ as
\begin{equation}
  \label{eq:soccp1-ter}
  - \left[\begin{array}{l}
    M v-H r-f \\
    u - H^\top v - w\\
   u + g(u)
 \end{array}\right]    \in N_{\bar K}\left(
\left[\begin{array}{l}
  v \\u \\r
\end{array}\right]
\right).
\end{equation}
Finally, the VI formulation of Problem~\ref{prob:I} is given by
\begin{equation}
  \label{eq:vi-I-bis}
  \left[\begin{array}{l}
    M v-H r-f \\
    u - H^\top v - w \\
    u + g(u )
  \end{array}\right] \left(
  \left[\begin{array}{l}
      p  \\ q \\ s
    \end{array}\right] - \left[\begin{array}{l}
      v  \\ u \\ r
    \end{array}\right]
\right) \geq 0,\quad \text{ for all } s \in K,  p \in \RR^n \text{ and }  q \in \RR^m.
\end{equation}
Let us introduce a convenient notation for this formulation into $\mathrm{VI}(F_{\vione},X_{\vione})$ with
\begin{equation}
  \label{eq:vi-I}
  F_{\vione}(u,v,r) = \left[\begin{array}{l}
    M v-H r-f \\
    u - H^\top v - w \\
    u + g(u)
  \end{array}\right],\quad \text{ and } X_{\vione} = \bar K = \RR^n \times \RR^m \times K\subset \RR^{2m+n}.
\end{equation}


For Problem~\ref{prob:II}, the following equivalent formulation in VI is directly obtained from
\begin{equation}
  \label{eq:inclusion-1}
  -(W r +q + g(Wr+q))  \in N_K(r).
\end{equation}
 The resulting VI is denoted by $\mathrm{VI}(F_{\vitwo},X_{\vitwo})$ with
\begin{equation}
  \label{eq:vi-II}
  F_{\vitwo}(r) = W r +q + g(Wr+q)\text{ and } X_{\vitwo} = K.
\end{equation}

\begin{ndrva}
  \begin{itemize}\item 
    Concerning the mathematical property of these formulation,
    ... monotonicity, co-positivity
  \end{itemize}
\end{ndrva}

{\blue 

\paragraph{monotonicity}

For Problem~\ref{prob:II},%  we have the VI (\ref{eq:vi-II}) that we rewrite for our convenience with
% \begin{equation}
%   \label{eq:vi-II}
%   F_{\vitwo}(u,r) =\left[
%   \begin{array}{c}
%     u - Wr -q
%     u + g(u)
% \end{array}\right]
% \text{ and } X_{\vitwo} = \RR^{n_c}\times K.
% \end{equation}
% \begin{equation}
%   \label{eq:mono-IIa}
%     (F_{\vitwo}(u,r)-F_{\vitwo}(v,s))^T(
%     \left[\begin{array}{c}
%         u \\ r
%     \end{array}\right]
% -
%  \left[\begin{array}{c}
%         v \\ s
%     \end{array}\right]
% ) = (r-s)^T W (r-s)   + \|u-v\|^2 + \sum _{\alpha =1}^{n_c} \mu^\alpha (x_\n-y_n) [\|[Wx+q]^\alpha_\t \| - \|[Wy+q]^\alpha_\t \|]
% \end{equation}





\begin{equation}
  \label{eq:mono-II}
    (F_{\vitwo}(x)-F_{\vitwo}(y))^T(x-y) = (x-y)^T W (x-y) + \sum _{\alpha =1}^{n_c} \mu^\alpha (x_\n-y_n) [\|[Wx+q]^\alpha_\t \| - \|[Wy+q]^\alpha_\t \|]
\end{equation}

\begin{equation}
  \label{eq:Jac-II}
    \nabla_r F_{\vitwo}(r) = W + W\left[
    \begin{array}{cc}
       0 & \mu \Frac{[W r+q]_\t}{\|[W r+q]_\t\|}\\
       0 & 0
    \end{array}\right]
\end{equation}



}


Let $X$ be a multi-valued mapping  $\nbR^{n} \rightsquigarrow  \nbR^{n}$ and let  $F$ be a mapping form $\nbR^n$ into itself. The  quasi-variational inequality  problem, denoted by $\mathrm{QVI}(X,F)$ is to find a vector $z\in\nbR^n$ such that 
\begin{equation}
 \label{eq:qvi}
 F^{T}(z) (y-z) \ge 0, \, \forall y \in X(z)
\end{equation}
The QVI formulations of the frictional contact problems are obtained by considering the inclusions~(\ref{eq:signo-inclusion-velo}) and~(\ref{eq:Coulomb-inclusion}). We get 
\begin{equation}
  \label{eq:qvi-1}
  u^\top (s - r) \geq 0, \text{ for all } s \in C(\mu, r_\n)
\end{equation}
where $C(\mu, r_\n)$ is the Cartesian product of the semi--cylinders of radius $\mu^\alpha r_n^\alpha$ defined by
\begin{equation}
  \label{eq:cylinder}
  C(\mu, r_\n) = \prod_{\alpha =1}^{n_c} \{ s \in \RR^3 \mid  s_\n \geq 0, \|s_\t\| \leq \mu^\alpha r^\alpha_n     \}
\end{equation}
Note that the QVI~(\ref{eq:qvi-1}) involves only $u$ and not $\hat u$; this is the main interest of formulating a QVI. The price to pay is the dependence on $r$ of the set $C(\mu,r_\n)$. The frictional contact problems~\ref{prob:I} and~\ref{prob:II}  can be expressed as a QVI by substituting the expression of $u$. For Problem~\ref{prob:II} we obtain
\begin{equation}
  \label{eq:qvi-IIbis}
   (Wr+q)^\top (s - r) \geq 0, \text{ for all } s \in C(\mu,r_\n).
\end{equation}
Since $W$ is assumed to be semi-definite positive matrix, the affine mapping is monotone, thus we get an affine monotone $\mathrm{QVI}(F_{\qvitwo},X_{\qvitwo})$ with
\begin{equation}
  \label{eq:qvi-II}
  F_{\qvitwo}(r) =  Wr+q \text{ and } X_{\qvitwo} = C(\mu,r_\n) .
\end{equation}


\subsection{Nonsmooth Equations}
\label{Sec:NonsmoothEquations}
In this section, we consider the reformulations of Problems~\ref{prob:I} and~\ref{prob:II} into various nonsmooth equations. More precisely for the problem of \ref{prob:I}, we search for an equation of the type
\begin{equation}
  \label{eq:ne-1}
  G(v,u,r) = 0
\end{equation}
where $G$ is nonsmooth (generally locally Lipschitz continuous) such that the zeroes $u,v,r$ of (\ref{eq:ne-1}) are the solutions of~(\ref{eq:soccp1}). Having in hand such a nonsmooth equations formulations, we will see further that several numerical methods can be used to solve it ranging from fixed point algorithms to Newton's method.

\subsubsection{Natural and normal maps for the VI formulations}

A first way to obtain some formulations based on nonsmooth equations of the frictional contact problems is to use the normal and natural maps (see \cite{Facchinei.Pang2003} for details) which enables to restate a VI into a system of nonlinear and  nonsmooth functions. Let us define the Euclidean projector $P_X$ onto a closed convex $X$. For a vector $x\in \RR^n$, the projected vector $\bar x  = P_X(x)$ is the unique solution of the convex program
\begin{equation}
  \label{eq:opt-proj}
  \begin{cases}
    \min\, \Frac 1 2 (y-x)^T(y-x), \\[2mm]
    \begin{array}{ll}
    s.t. & y \in X .
  \end{array}
  \end{cases}
\end{equation}
 The natural map $F^\nat$ associated with the VI (\ref{eq:vi}) is defined by
\begin{equation}
  \label{eq:naturalmap}
  F^\nat(z) = z - P_{X}(z-F(z)),
\end{equation}
and  the normal map $F^\nor$ is defined by
\begin{equation}
  \label{eq:normalmap}
  F^\nor(z) = F(P_X(z)) + z - P_{X}(z).
\end{equation}
A well-known result relates the solution of the VI and the zeroes of the natural and normal map (see \cite{Facchinei.Pang2003}) such that
\begin{equation}
  \label{eq:VI-normalnatural}
  \begin{array}{c}
  x \text{ solves } \mathrm{VI}(X,F) \Longleftrightarrow  F^\nat(x)=0 \\[2mm]
  x \text{ solves } \mathrm{VI}(X,F) \Longleftrightarrow x= P_X(z) \text{ for some } z \text{ such that } F^\nor(z)=0.
\end{array}
\end{equation}
Note that the if $x$ solves  $\mathrm{VI}(X,F)$, it solves also  $\mathrm{VI}(X,\rho F)$ for $\rho > 0$. Therefore, we can define some parameterized normal and natural map by
\begin{equation}
  \label{eq:parametrized-naturalnormalmap}
  \begin{array}{l}
    F_\rho^\nat(z) = z - P_{X}(z-\rho F(z)), \\
    F_\rho^\nor(z) = \rho F(P_X(z)) + z - P_{X}(z).
  \end{array}
\end{equation}
for which the equivalence (\ref{eq:VI-normalnatural}) continues to hold. 
 

Using these equivalent nonsmooth equations, the frictional contact problems can be restated as zeroes of nonsmooth functions. Considering the natural map, we obtain for Problem~\ref{prob:I} under the VI form (\ref{eq:vi-I}),
\begin{equation}
  \label{eq:natural-I}
  F_\vione^\nat(v,u,r) = \left[
  \begin{array}{l}
    \rho (Mv - Hr + f) \\
    \rho (u-H^\top v -w) \\
    r - P_{K}\left(r  - \rho (u  + g(u))\right)
  \end{array}\right] = 0,
\end{equation}
and  for Problem~\ref{prob:II} under the VI form (\ref{eq:vi-II}),
\begin{equation}
  \label{eq:natural-II}
  F_\vitwo^\nat(u,r) =   \left[
  \begin{array}{l} 
    u - W r -q\\
    r - P_{K}\left(r  - \rho (u + g(u))\right) 
  \end{array}\right] 
  = 0.
\end{equation}
The normal map based formulation are also obtained in the same way. 


More generally, given a symmetric positive definite matrix $D\in R^{n\times n}$, a skewed projector $P_{X,D}$ onto $X$ can be defined. For a vector $x\in \RR^n$, the skew projected vector $\bar x  = P_{X,D}(x)$ is the unique solution of the convex program
\begin{equation}
  \label{eq:opt-proj-skew}
  \begin{cases}
    \min\, \Frac 1 2 (y-x)^T D (y-x), \\[2mm]
    \begin{array}{ll}
    s.t. & y \in X .
  \end{array}
  \end{cases}
\end{equation} 
The skew natural map can be also defined and yield the following nonsmooth equation
\begin{equation}
  \label{eq:skew-natural-NE}
   F_D^\nat(z) = z - P_{X,D}(z-D^{-1} F(z)).
\end{equation}
Considering the skew natural map, we obtain for Problem~\ref{prob:I} under the VI form (\ref{eq:vi-I}),
\begin{equation}
  \label{eq:natural-typeI}
  F_{\vione,D}^\nat(v,u,r) = \left[
  \begin{array}{l}
    D_1^{-1} (Mv - Hr + f) \\ 
    D_2^{-1} (u-H^\top v -w) \\
    r - P_{K}\left(r  - D_3^{-1} (H^\top v + w  + g(H^\top v + w))\right)
  \end{array}\right] = 0
\end{equation}
with $D_1\in \RR^{n\times n}$, $D_2\in \RR^{m\times m}$ and $D_3\in \RR^{m\times m}$.
For Problem~\ref{prob:II} under the VI form (\ref{eq:vi-II}),
\begin{equation}
  \label{eq:natural-typeII}
  F_{\vitwo,D}^\nat(u,r) = \left[
  \begin{array}{l} 
    \rho(u - W r -q)\\
    r - P_{K}\left(r  - D^{-1} (u   + g(u))\right)  \end{array}\right] = 0
\end{equation}
The previous case is retrieved by choosing $D = 1/ \rho I_{n\times n}$.
\begin{ndrva}
  It can be interesting to use something $D$ as a preconditionner of the problem ? $D=diag(W)$ or incomplete LU. Warning, $W$ is only SPD, so we cannot have $D =W$.
\end{ndrva}

\subsubsection{Alart--Curnier's  function and variants}

In the previous section, the projection operator onto the Coulomb cone $P_K$ plays an important role. If we use the alternative inclusions formulations~(\ref{eq:signo-inclusion-velo})--~(\ref{eq:Coulomb-inclusion}) with a given parametrization such that
\begin{equation} 
  \label{eq:inclusion-rhorho}
  \left\{ \begin{array}{ll}
    -\rho_\n u_\n \in N_{\RR^{n_c}_+}(r_\n), & \rho_\n>0, \\
    -\rho_\t u_{\t} \in N_{D(\mu r_{\n,+})}(r_\t), & \rho_\t>0,
  \end{array}\right.
\end{equation}
 we can split $P_K$ into $P_{\RR^{n_c}_+}$ and $P_{D(\mu,r_{n,+})}$ where 
\begin{equation}
  \label{eq:diskR-prod}
  D(\mu,r_{n,+}) = \prod_{\alpha=1\ldots n_c} D(\mu^{\alpha} r_{n,+}^\alpha).
\end{equation}
defines the Cartesian product of the Coulomb disks for each contact. The notation $x_+$ stands for $x_+ = \max(0,x)$. This procedure yields
\begin{equation}
  \label{eq:CKPS-1}
    \begin{cases}
    r_\n - P_{\RR^{n_c}_+}(r_\n - \rho_\n  u_\n) = 0, \\
    r_\t - P_{D(\mu, r_{\n,+})}(r_\t - \rho_\t u_\t   )=0,
  \end{cases}
\end{equation}
The parameters $\rho_n,\rho_\t$ may be chosen contact by contact and yield therefore to some vectors of parameters.


An analog formulation to~(\ref{eq:CKPS-1}) was originally introduced in the seminal work of  Alart \& Curnier~\cite{Curnier.Alart88,Alart.Curnier1991} to design a nonsmooth (or generalized) Newton method (see Sect.~\ref{Sec:NSN-AC} ). This formulation, motivated by an augmented Lagrangian approach, is
\begin{equation}
  \label{eq:AC-1}
  \begin{cases}
    r_\n - P_{\RR^{n_c}_+}(r_\n - \rho_\n  u_\n) = 0, \\
    r_\t - P_{D(\mu, (r_\n - \rho u_\n)_+)}(r_\t - \rho_\t u_\t   )=0.
  \end{cases}
\end{equation}

\begin{ndrva}
  understand the continuity comment of Alart.
\end{ndrva}



%  In this section, we use  another description of the Coulomb cone based on the Coulomb disk. Let us start by recalling the following inclusion
% By splitting the projection onto the Coulomb cone in a projection onto $\RR_+$ and a projection onto $D(\mu^\alpha r_\n^\alpha)$, we obtain the following equivalent problems
% \begin{equation}
%   \label{eq:AC-1bis}
%   \begin{array}{c}
%     K^\alpha \ni \hat u^\alpha \perp r^\alpha \in K^\alpha \\
%     \Updownarrow \\
%      r^\alpha - P_K(r^\alpha-\rho u^\alpha) =0, \rho >0 \\
%      \Updownarrow \\
%      \begin{cases}
%        r^\alpha_\n - P_{\RR_+}(r^\alpha_\n - \rho  u^\alpha_\n) = 0 \\
%        r^\alpha_\t - P_{D(\mu^\alpha(r^\alpha_\n - \rho \hat u^\alpha_\n))}(r^\alpha_\t - \rho u^\alpha_\t   )=0
%      \end{cases}\\
%   \end{array}
% \end{equation}

Problem~\ref{prob:I} is then reformulated as
\begin{equation}
  \label{eq:AC-I}
  F_{\acone}(v,u,r) = \left[
    \begin{array}{c}
    Mv - Hr - f \\
    u -  H^\top v - w \\
    r_\n - P_{\RR^{n_c}_+}(r_\n - \rho_\n u_\n) \\
    r_\t - P_{D(\mu, (r_\n - \rho_\n u_\n)_+)}(r_\t - \rho_\t u_\t   )\\[2mm] \text{ (or  } r_\t - P_{D(\mu, r_{\n,+})}(r_\t - \rho_\t u_\t   ) \text{)}
  \end{array}\right] =0
\end{equation}
and  Problem~\ref{prob:II}  as
\begin{equation}
  \label{eq:AC-II}
    F_{\actwo}(u,r) = \left[ \begin{array}{c}
    u - W r -  q  \\
    r_\n - P_{\RR^{n_c}_+}(r_\n - \rho_\n u_\n) \\
    r_\t - P_{D(\mu, (r_\n - \rho_\n u_\n)_+)}(r_\t - \rho_\n u_\t   )\\[2mm]\text{ (or  } r_\t - P_{D(\mu, r_{\n,+})}(r_\t - \rho_\t u_\t   ) \text{)}
  \end{array}\right] =0.
\end{equation}

\begin{remark}
  From the QVI formulation~(\ref{eq:qvi-1}), the following nonsmooth
  equation can also be written
  \begin{equation}
    \label{eq:qvi-proj}
    r = P_{C(\mu,r_\n)}(r-\rho u)
  \end{equation}
  which exactly corresponds to~(\ref{eq:CKPS-1}). \texttt{ A verifier}
\end{remark}




\subsubsection{General C-function for SOCCP}
\begin{ndrva}
  This is perhaps  too much
\end{ndrva}

More generally, a large family of functional reformulations of the SOCCP~(\ref{eq:contact-SOCCP}) can be given  by using a so-called Second Order Cone Complementarity (SOCC) function. A SOCC-function $\phi$ is defined by
\begin{equation}
  \label{eq:SOCC-function}
  K^\star \ni x \perp y \in K \Longleftrightarrow \phi(x,y)=0.
\end{equation}
Clearly, the nonsmooth equations of the previous sections provides several examples of SOCC-functions and the natural map offers the most simplest SOCC-function.

In~\cite{Fukushima.ea2001}, the standard complementarity functions for Nonlinear Complementarity Problems (NCP) such as the min function and the Fischer-Burmeister function are extended to the SOCCP by means of Jordan algebra. Smoothing functions are also given with theirs Jacobians and they studied their properties in view of the application of Newton's method. 

For the second order cone defined in (\ref{eq:CoulombCone}), the Jordan algebra can be defined with the following non-associative Jordan product,
\begin{equation}
  \label{eq:Jordan-product}
  x \cdot y =
  \left[\begin{array}{c}
      x^\top y \\
      \mu y_\n x_\t + \mu x_\n y_\t
  \end{array}
\right]
\end{equation}
and the usual componentwise addition $x+y$. The vector $x^2$ denotes $x\cdot x$ and their exists an unique vector $x^{1/2}\in K$ for all $x \in K$ such that
\begin{equation}
  \label{eq:Jordan-sqrt}
  (x^{1/2})^2 = x^{1/2} \cdot x^{1/2} = x.
\end{equation}
A direct calculation yields
\begin{equation}
  \label{eq:Jordan-sqrt2}
  x^{1/2} =
  \left[\begin{array}{c}
    s \\
    \Frac {x_\t} {2s}
  \end{array}\right], \quad \text{ where } s = \sqrt{(x_\n+\sqrt{x_\n^2 - \|x_\t\|^2})/2}
\end{equation}
If $x=0$, we can remark that $x_\t =0$ and then $s=0$. In this case, $x/2s$ is defined to be zero, that is $x^{1/2}=0$. The vector $|x| \in K$ denotes $(x^2)^{1/2}$.


\begin{ndrva}
  Finally, note that
  \begin{equation}
    \label{eq:equiv}
    K^* \ni x \perp y \in K \Longleftrightarrow K^* \ni x \cdot y \in K
  \end{equation}
Check there is no troubles with $\mu =0$. This has to be done for all of this section.
\end{ndrva}

Thanks to this algebra and its associated operator, the projection onto $K$ can be written as
\begin{equation}
  \label{eq:Jordan-projection}
  P_K(x) = \Frac{x + |x|}2.
\end{equation}
This formula provides a new expression for the natural map and its associated nonsmooth equations. More interesting is the fact that more complicated C-functions can be extended and smoothed version of this function can be also developed. Let us start with the Fischer-Burmeister function for NCP that can be written
\begin{equation}
  \label{eq:Jordan-FB}
  \phi_{\fb}(x,y) = x+y - (x^2 + y^2 )^{1/2}
\end{equation}
and its smoothed version with a regularization parameter $\mu>0$ as
\begin{equation}
  \label{eq:Jordan-FB-smoothed}
  \phi_{\fb, \mu}(x,y) = x+y - (x^2 + y^2 + 2 \mu^2 e)^{1/2}
\end{equation}
where $e$ is the identity element of the Jordan algebra, that is $e \cdot x =x$. In the same vein, the class of smoothing function of the natural map for NCP developed in \cite{Chen.Mangasarian1996} is extended to SOCCP in~\cite{Fukushima.ea2001}.

In \cite{Hayashi.ea2005}, the detailed analysis is simpler ....
\begin{ndrva}
  TBW
\end{ndrva}

\cite{Zhang.ea2009}




\subsection{Others complementarity formulations (as Remarks)}
Kanno's approach\cite{Kanno.ea2006}
\begin{ndrva}
  TBW
\end{ndrva}

\subsection{Optimization problems}

In this section, several optimization-based formulations are proposed. The quest for an efficient optimization formulation of the frictional problem is an hard task. Since the problem is nonsmooth and nonconvex, the use of an associated optimization problem is interesting from the numerical point of view if we want to improve the robustness and the stability of the numerical methods.

%\subsubsection{A direct optimization problem}

 A straightforward optimization problem can be written whose cost function to minimize is the scalar product $r^\top\hat u$. Indeed, this product is always  positive and vanishes at the solution. Let us consider this first optimization formulation
\begin{equation}
  \label{eq:opt-1}
  \begin{cases}
    \min\,  r^\top \hat u = r^\top u + \sum _{\alpha=1}^{n_c} \mu^\alpha r_\n^\alpha \|u_\t^\alpha\| \\[2mm]
    \begin{array}{ll}
    s.t. &\hat u \in K^\star, \\
    &r \in K,
  \end{array}
  \end{cases}
\end{equation}
which amounts to minimizing the DeSaxc\'e's bipotential function~\cite{DeSaxce92} over $K^\star\times K$. A first simplification can be made by noting that
\begin{equation}
  \label{eq:equiv-cone}
  \hat u \in K^\star \Longleftrightarrow u_{\n} \geq 0, 
\end{equation}
which leads to
\begin{equation}
  \label{eq:opt-2}
  \begin{cases}
    \min\, r^\top u + \sum _{\alpha=1}^{n_c} \mu^\alpha r_\n^\alpha \|u_\t^\alpha\| \\[2mm]
     \begin{array}{ll}
    s.t. &u_\n \geq 0\\
    &r \in K.
  \end{array}
  \end{cases}
\end{equation}


Starting from Problem~\ref{prob:II}, a direct substitution of $u = Wr +q$ yields
\begin{equation}
  \label{eq:opt-3}
  \begin{cases}
    \min\, r^\top (Wr+q) + \sum _{\alpha=1}^{n_c} \mu^\alpha r_\n^\alpha \|(Wr+q)_\t^\alpha\| \\[2mm]
    \begin{array}{ll}
      s.t. &(W r + q){\sf N} \geq 0, \\
      &r \in K.
    \end{array}
  \end{cases}
\end{equation}
which is an nonlinear optimization problem with a nonsmooth and nonconvex cost function.  From the numerical point of view this problem may be very difficult and we have to ensure that the cost function have to be zero at the solution which is not guaranteed of some local minima are reached in the minimization process.

Other optimization-based formulations have been proposed in the literature. They are not direct optimization formulation but they try to identify a optimization sub-problem which is well-posed and for which efficient numerical methods are available. Three approaches can be listed in three categories : a)  the \textit{alternating optimization} problems, b) the \textit{successive approximation} method and c)  the \textit{convex SOCP} approach.


\paragraph{The alternating optimization problems} aims at solving the frictional contact problem by alternatively solving the Signorini condition for a known fixed values of the tangential reaction and solving the Coulomb friction model for a known fixed values of the normal reaction. 

For Problem~\ref{prob:II}, let us consider the following splitting of the matrix $W$ and the vector $q$
\begin{equation}
  \label{eq:W-split}
  u = W r +q \Longleftrightarrow 
  \left[\begin{array}{c}
    u_\n \\ u_\t
  \end{array}\right]
 =
  \left[\begin{array}{cc}
    W_{\n\n} &W_{\n\t} \\
    W_{\t\n} &W_{\t\t} \\    
  \end{array}\right]    \left[\begin{array}{c}
    r_\n\\ r_\t
  \end{array}\right]+
 \left[\begin{array}{c}
    q_\n \\q_\t
  \end{array}\right].
\end{equation}
Two sub-problems can therefore be identified : the first problem for the normal part is to find $u_\n$ and $r_\n$ such that
\begin{equation}
  \label{eq:AO-1}
  \begin{cases}
    u_\n = W_{\n\n} r_\n + \tilde q_\n, \\
    0\leq u_\n \perp r_\n \geq 0,
  \end{cases}
\end{equation}
with a given value of $\tilde q_\n = q_\n + W_{\n\t}r_\t$ and the second problem for the tangent part is to find $u_\t$ and $r_\t$ such that
\begin{equation}
  \label{eq:AO-2}
  \begin{cases}
    u_\t = W_{\t\t} r_\t + \tilde q_\t, \\
   -u_\t \in N_{D(\mu,\tilde r_{\n})}(r_\t),
  \end{cases}
\end{equation}
with  given values of $\tilde r_\n$ and $\tilde q_\t = q_\t + W_{\t\n}r_n$. Since $W$ is a symmetric semi--definite positive matrix, $W_{\n\n}$ and $W_{\t\t}$ are also symmetric semi--definite positive matrices. Two convex optimization problems can therefore be formulation as
 \begin{equation}
  \label{eq:AO-3}
  \begin{cases}
    \min\, \Frac 1 2 r_\n^\top  W_{\n\n} r_\n + r_n^\top \tilde q_\n, \\
    \begin{array}{ll}
    \text{s.t.} & r_\n \geq 0,
  \end{array}\end{cases}
\end{equation}
and
\begin{equation}
  \label{eq:AO-4}
  \begin{cases}
    \min\,  \Frac 1 2 r_\t^\top W _{\t\t} r_\t +  r_\t^\top \tilde q_\t, \\
    \begin{array}{ll}
     \text{s.t.}  & r_\t \in D(\mu,\tilde r_{\n}).
  \end{array}
\end{cases}
\end{equation}
Problem~\ref{prob:I} is treated in the same manner by splitting the matrix $H$ and the vector $w_\n$ such that
\begin{equation}
  \label{eq:H-split}
  u = H^\top v +w \Longleftrightarrow  \left[\begin{array}{c}
    u_\n \\ u_\t
  \end{array}\right]
 =
  \left[\begin{array}{c}
      H_{\n} \\
      H_{\t} \\    
    \end{array}\right]^\top   \, v+
 \left[\begin{array}{c}
    w_\n \\w_\t
  \end{array}\right].
\end{equation}
We get for the normal part
\begin{equation}\label{eq:soccp1-normal}
  \begin{cases}
    M v = {H_\n} {r_\n} + {\tilde f_\n} \\[2mm]
    u_\n = H_\n^\top v + w_\n \\[2mm]
    0 \leq { u_\n} \perp r _\n \geq 0
  \end{cases}
\end{equation}
for  given value of $\tilde r_\t$ and $\tilde f_\n = f + H_\t r_\t$. Since $M$ is a symmetric positive definite matrix, an equivalent convex optimization can be written
\begin{equation}
  \label{eq:soccp1-normal-min}
    \begin{cases}
    \min\, \frac 1 2 v^\top M v + v^\top \tilde f_\n  \\[2mm]
     \begin{array}{ll}
    s.t. & H_\n^\top v + w_\n \geq 0
  \end{array}
  \end{cases}
\end{equation}
and $u_\n$ is obtained by $ u_n = H_\n^\top v + w_\n $ and $r_\n$ is the Lagrange multiplier associated with the constraints. For the tangent part, the situation is a little bit more complicated since we get
\begin{equation}\label{eq:soccp1-tangent}
  \begin{cases}
    M v = {H_\t} {r_\t} + {\tilde f_\t} \\[2mm]
    u_\t = H_\t^\top v + w_\t \\[2mm]
    -u_\t \in N_{D(\mu,\tilde r_{\n})}(r_\t),
  \end{cases}
\end{equation}
8for  given value of  $\tilde r_\n$ and $\tilde f_\t = f + H_\n \tilde r_\n$. It is therefore difficult to associate an optimization which differs from~(\ref{eq:AO-4}).  

%  Since $M$ is a symmetric positive definite matrix, an equivalent convex optimization can be written
% \begin{equation}
%   \label{eq:soccp1-tangent-min}
%     \begin{cases}
%     \min\, \frac 1 2 v^\top M v + v^\top \tilde f_\t  \\[2mm]
%      \begin{array}{ll}
%     s.t. & u_\t = H_\t^\top v + w_\t
%   \end{array}
%   \end{cases}
% \end{equation}
% and $u_\n$ is obtained by $ u_n = H_\n^\top v + w_\n $ and $r_\n$ is the Lagrange multiplier associated with the constraints.


\texttt{citer Panagiotopoulos}

\paragraph{The successive approximation} method identifies a single optimization problem by introducing a function that maps the normal reaction to itself (or the friction threshold) such that
\begin{equation}
  \label{eq:Haslinger-g}
  h(r_n)  = r_\n
\end{equation}
Using this artifact, we can define a new problem from Problem~\ref{prob:II} such that
\begin{equation}
  \label{eq:Haslinger-1}
  \begin{cases}
    \theta = h(r_\n) \\
    u = W r + q \\
    -u_{\n} \in N_{\RR^{n_c}_+}(r_\n) \\
    -u_{\t} \in N_{D(\mu,\theta)}(r_\t)
  \end{cases}
\end{equation}
Since W is a symmetric semi-definite positive matrix, the last three lines are equivalent to a convex optimization problem over the product of cylinder $C(\mu,\theta)$, that is
\begin{equation}
  \label{eq:Haslinger-2}
  \begin{cases}
    \theta = h(r_\n) \\[2mm]
    \min\, \Frac 1 2 r^\top W r + r^\top q \\
    \begin{array}{ll}
    \text{s.t. }& r \in C(\mu,\theta)
  \end{array}
  \end{cases}
\end{equation}

The method of successive approximation has been extensively used for proving existence and uniqueness of solutions to the discrete frictional contact problems. We refer to~\cite{Haslinger.ea1996} which summarizes the seminal work of the Czech school~\cite{Necas.ea1980,Haslinger1983,Haslinger1984}. We will see in the sequel that this approach also provides us with very efficient numerical solvers.



\paragraph{ The convex SOCP} approach operates in the same vein but a  SOCQP sub-problem is identified. To this aim, we use the function $g(u)$ introduced in~(\ref{eq:modified-bis}) and thanks to this mapping we rewrite Problem~\ref{prob:II} as
\begin{equation}\label{eq:ACLM-2}
  \begin{cases}
    s = g(u) \\[2mm]
    \hat u = W r + q + s  \\[2mm]
    K^\star \ni {\hat u} \perp r \in K
  \end{cases}
\end{equation} 
Since $W$ is a positive semi-definite matrix, a new convex optimization sub-problem can be defined 
\begin{equation}\label{eq:ACLM-3}
  \begin{cases}
    s = g(u) \\[2mm]
    \min\,\Frac 1 2 r^\top W r + r^\top (q + s)  \\
    \begin{array}{ll}
    \text{s.t. } & r \in K
  \end{array}
  \end{cases}
\end{equation} 
Problem~\ref{prob:I} ....

 
This formulation introduced in~\cite{cadoux-2009,Acary.ea2010} has been used to give an existence criteria to the discrete frictional contact problems. Furthermore, this existence criteria can be numerically checked. We will see in the sequel what could be its interest for solving the problems, we are concerned.

\section{Numerical methods for VIs}



\subsection{Basic fixed point algorithms}

Starting from the VI formulations~(\ref{eq:vi}) or more precisely an associated nonsmooth equation through the natural map,
\begin{equation}
  \label{eq:skew-natural-NE-bis}
   F_D^\nat(z) = z - P_{X,D}(z-D^{-1} F(z)).
\end{equation}
A basic idea of algorithm is to perform fixed point iterations on the mapping
\begin{equation}
  \label{eq:skew-fixed-point}
   z \mapsto P_{X,D}(z-D^{-1} F(z)).
\end{equation}
yielding to Algorithm~\ref{Algo:FP-vi}.
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf F,X$ Data of VI~(\ref{eq:vi})
      \REQUIRE $\sf D$ a symmetric positive definite matrix
      \REQUIRE $\sf z_0$ initial values and $\sf tol >0$ a tolerance
      \ENSURE  $\sf z$ solution of VI~(\ref{eq:vi})
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE $\sf z_{k+1} \leftarrow P_{X,D}(z_k - D^{-1}\,F(z_k))$
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$
      \ENDWHILE
      \STATE $\sf z \leftarrow z_{k}$ 
    }
  \end{algorithmic}
  \caption{Fixed point iterations for the VI~(\ref{eq:vi})}  \label{Algo:FP-vi}
\end{algorithm}


Applying the Algorithm~\ref{Algo:FP-vi} to the VI formulation (\ref{eq:vi-I}) leads to the following iteration
\begin{equation}
  \label{eq:FP-vi-I}
  \begin{cases}
    v_{k+1} \leftarrow  v_k - D_1^{-1}( M v_{k} - H r_k +f) \\
    u_{k+1} \leftarrow  u_k - D_2^{-1}( u_k - H^\top v_{k} - w) \\ 
    r_{k+1} \leftarrow  P_{K,D_3}(r_k - D_3^{-1}(u_k +  g(u_k))).
  \end{cases}
\end{equation}
Due to the fact that $u$ is explicitly given as a function of $v$, $D_2$ is chosen as the identity matrix and $u_{k+1}$ plays the role of $u_k$ in the last equation. The following iteration is obtained
\begin{equation}
  \label{eq:FP-vi-Ibis}
  \begin{cases}
    v_{k+1} \leftarrow  v_k - D_1^{-1}( M v_{k} - H r_k +f) \\
    r_{k+1} \leftarrow  P_{K,D_3}(r_k - D_3^{-1}(H^\top v_k + w +  g(H^\top v_k+ w ))).
  \end{cases}
\end{equation}
For the formulation (\ref{eq:vi-II}), the following iterations are performed
\begin{equation}
  \label{eq:FP-vi-II}
  %\begin{cases}
       r_{k+1} \leftarrow  P_{K,D}(r_k - D^{-1}( W r_k + q +  g(W r_k + q ))).
  %\end{cases}
\end{equation}
 In the sequel when a parameter $\rho$ is specified, it is assumed that $D$ is chosen as $1/\rho$ times the identity matrix.


% Algorithms~\ref{Algo:FP-vi-I} and~\ref{Algo:FP-vi-I}.
% \begin{algorithm}
%   \label{Algo:FP-vi-I}
%   \begin{algorithmic}
%     {\sf
%       \STATE $ $
%       \REQUIRE $\sf M,H,f,w,\mu$ Data of Problem~\ref{prob:I}
%       \REQUIRE $\sf D_1,D_2$ two symmetric positive definite matrices
%       \REQUIRE $v_0,r_0$ initial values and $\sf tol >0$ a tolerance
%       \ENSURE  $\sf u,v,r$ solution of Problem~\ref{prob:I}
%       \STATE   $\sf k \leftarrow 0$ 
%       \WHILE {$\sf error > tol$} 
%       \STATE $\sf v_{k+1} \leftarrow  v_k - D_1^{-1}( M v_{k} - H r_k +f)$
%       \STATE $\sf r_{k+1} \leftarrow P_{K,D_2}(r_k - D_2^{-1}(H^\top v_k + w  + g(H^\top v_k + w)))$
%       \STATE Evaluate $\sf error$.
%       \STATE $\sf k \leftarrow k+1$
%       \ENDWHILE
%       \STATE $\sf v \leftarrow v_{k},\quad \sf r \leftarrow r_{k}$ 
%       \STATE $\sf u \leftarrow H^\top v_k + w $
%     }
%   \end{algorithmic}
%   \caption{Fixed point iterations on Problem~\ref{prob:I} based on VI~(\ref{eq:vi-I})}
% \end{algorithm}
% \begin{algorithm}
%   \label{Algo:FP-vi-II}
%   \begin{algorithmic}
%     {\sf
%       \STATE $ $
%       \REQUIRE $\sf W,q,\mu$ Data of Problem~\ref{prob:II}
%       \REQUIRE $\sf D$ a symmetric positive definite matrix
%       \REQUIRE $r_0$ initial values and $\sf tol >0$ a tolerance
%       \ENSURE  $\sf u,r$ solution of Problem~\ref{prob:II}
%       \STATE   $\sf k \leftarrow 0$ 
%       \WHILE {$\sf error > tol$} 
%       \STATE $\sf r_{k+1} \leftarrow P_{K,D_2}(r_k - D^{-1}(W r_k + q + g(W r_k + q)))$
%       \STATE Evaluate $\sf error$.
%       \STATE $\sf k \leftarrow k+1$
%       \ENDWHILE
%       \STATE $\sf  r \leftarrow r_{k}$ 
%       \STATE $\sf u \leftarrow H^\top v_k + w $
%     }
%   \end{algorithmic}
%   \caption{Fixed point iterations on Problem~\ref{prob:II} based on VI~(\ref{eq:vi-II})}
% \end{algorithm}
The convergence of such methods are generally demonstrated for strongly monotone VI. For the problems we are concerned, this assumption is not satisfied, but we will see in the sequel that such methods can converge in practical numerical applications. 
Algorithm~\ref{Algo:FP-vi} with the iteration rule~(\ref{eq:FP-vi-II}) has been originally proposed in \cite{DeSaxce.Feng1998}. The algorithm is called Uzawa's algorithm by reference to the algorithm due to Uzawa in computing the optimal values of convex program by primal-dual techniques.



Using the other nonsmooth equations~(\ref{eq:CKPS-1}) and (\ref{eq:AC-1}) or the QVI formulation~(\ref{eq:qvi-II}), similar algorithms can be derived. For QVI~(\ref{eq:qvi}), we perform fixed point iterations on the mapping
\begin{equation}
  \label{eq:skew-fixed-point-qvi}
   z \mapsto P_{X(z),D}(z-D^{-1} F(z)).
\end{equation}
yielding to Algorithm~\ref{Algo:FP-qvi}. 
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf F,X$ Data of Problem~\ref{eq:vi}
      \REQUIRE $\sf D$ a symmetric positive definite matrix
      \REQUIRE $z_0$ initial values and $\sf tol >0$ a tolerance
      \ENSURE  $\sf z$ solution of Problem~\ref{eq:vi}
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE $\sf z_{k+1} \leftarrow P_{X(z_k),D}(z_k - D^{-1}\,F(z_k))$
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$
      \ENDWHILE
      \STATE $\sf z \leftarrow z_{k}$ 
    }
  \end{algorithmic}
  \caption{Fixed point iterations for the QVI~(\ref{eq:qvi})}  \label{Algo:FP-qvi}
\end{algorithm}
For Problem~\ref{prob:II} and~(\ref{eq:CKPS-1}), we get the following iteration rule
\begin{equation}
  \label{eq:FP-CKPS-II}
  \begin{cases}
    u_{k+1} \leftarrow W r_{k} +q\\
    r_{\n,k+1} \leftarrow P_{\RR^{n_c}_+}(r_{\n,k} -\rho_\n u_{\n,k+1}) \\
    r_{\t,k+1} \leftarrow P_{D(\mu,r_{\n,k})}(r_{\t,k}-\rho_\t u_{\t,k+1})
  \end{cases}
\end{equation}
and with the ~(\ref{eq:AC-1}), we get
\begin{equation}
  \label{eq:FP-AC-II}
  \begin{cases}
    u_{k+1} \leftarrow W r_{k} +q\\
    r_{\n,k+1} \leftarrow P_{\RR^{n_c}_+}(r_{\n,k} -\rho_\n u_{\n,k+1}) \\
    r_{\t,k+1} \leftarrow P_{D(\mu,r_{\n,k}-\rho_\n u_{\n,k+1} )}(r_{\t,k}-\rho_\t u_{\t,k+1})
  \end{cases}
\end{equation}
We will see in Section~\ref{Sec:OptimisationBasedMethods} that more evolved algorithms can be written exploiting the structure of the convex optimization sub-problems.


% Algorithm~\ref{Algo:FP-CKPS-II}. For the tangential part, the projection is performed on $D(\mu,r_{\n,k})$ at the iteration $k$. It can be also performed on  $D(\mu,r_{\n,k+1})$. We will see in Section~\ref{Sec:OptimisationBasedMethods} that more evolved algorithms can be written exploiting the convex optimization sub-problems
% \begin{algorithm}
%   \label{Algo:FP-CKPS-II}
%   \begin{algorithmic}
%     {\sf
%       \STATE $ $
%       \REQUIRE $\sf W,q,\mu$ Data of Problem~\ref{prob:II}
%       \REQUIRE $\sf \rho_\n,\rho_t$ user-defined (scalar or vector) parameters
%       \REQUIRE $r_0$ initial values and $\sf tol >0$ a tolerance
%       \ENSURE  $\sf u,r$ solution of Problem~\ref{prob:II}
%       \STATE   $\sf k \leftarrow 0$ 
%       \WHILE {$\sf error > tol$} 
%       \STATE $\sf u_{k+1} \leftarrow W r_{k} +q $
%       \STATE $\sf r_{\n,k+1} \leftarrow P_{\RR^{n_c}_+}(r_{\n,k} -\rho_\n u_{\n,k+1})$
%       \STATE $\sf r_{\t,k+1} \leftarrow P_{D(\mu,r_{\n,k})}(r_{\t,k} -\rho_\t u_{\t,k+1})$
%       \STATE Evaluate $\sf error$.
%       \STATE $\sf k \leftarrow k+1$
%       \ENDWHILE
%       \STATE $\sf  r \leftarrow r_{k}$ 
%       \STATE $\sf u \leftarrow W r_{k} +q$
%     }
%   \end{algorithmic}
%   \caption{Fixed point iterations on Problem~\ref{prob:II} based on nonsmooth equations~(\ref{eq:CKPS-1})  }
% \end{algorithm}
\begin{ndrva}
  More interesting in alternating context because we have two convex sub-problems.
\end{ndrva}

\begin{table}
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    { Algorithm}
    & Description 
    & parameters\\
    \hline
    \sf FP-VI-1 
    &  Algorithm~\ref{Algo:FP-vi} with the iteration rule~(\ref{eq:FP-vi-Ibis})
    & $D$ or $\rho$\\
    \hline
    \sf FP-VI-2 
    &  Algorithm~\ref{Algo:FP-vi} with the iteration rule~(\ref{eq:FP-vi-II})
    & $D$ or $\rho$\\
    \hline
    \sf FP-QVI-ACI-2 
    &  Algorithm~\ref{Algo:FP-qvi} with the iteration rule~(\ref{eq:FP-CKPS-II})
    & $\rho_\n,\rho_\t$\\
     \hline
    \sf FP-QVI-ACI-2 
    &  Algorithm~\ref{Algo:FP-qvi} with the iteration rule~(\ref{eq:FP-AC-II})
    & $\rho_\n,\rho_\t$\\
   \hline    
  \end{tabular}
  \caption{Naming convention for the algorithms}
  \label{tab:FP-algos}
\end{table}

\begin{ndrva}
  situate the work of \cite{DeSaxce.Feng90} and \cite{Simo.Laursen1992,Laursen.Simo1993b}.
\end{ndrva} 


\subsection{Projection methods for VI}

The extragradient method \citep{Korpelevich1976} is also a well-known method for VI which improves the previous projection method.  It can be described as
\begin{equation}
  \label{eq:vi-ge5}
  z_{k+1} \leftarrow P_X(z_{k}-\rho\,F(P_X(z_k-\rho F(z_k))))
\end{equation}
and formulated in Algorithm~\ref{Algo:ExtraGradient-vi}.
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf F,X$ Data of Problem~\ref{eq:vi}
      \REQUIRE $\sf D$ a symmetric positive definite matrix
      \REQUIRE $z_0$ initial values and $\sf tol >0$ a tolerance
      \ENSURE  $\sf z$ solution of Problem~\ref{eq:vi}
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE $\sf z_{k+1} \leftarrow P_X(z_{k}-\rho\,F(P_X(z_k-\rho F(z_k))))$
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$
      \ENDWHILE
      \STATE $\sf z \leftarrow z_{k}$ 
    }
  \end{algorithmic}
  \caption{Extragradient method for the VI~(\ref{eq:vi})}  \label{Algo:ExtraGradient-vi}
\end{algorithm}
The convergence of this method requires  that the function $F$ is Lipschitz--continuous and pseudo--monotone and that a solution exists. This method has been further extended in \citep{Solodov.Tseng1996} by
\begin{equation}
  \label{eq:VI-Proj3}
  z_{k+1}\leftarrow z_k-\tau P^{-1}\left[T_{\rho}(z_k)-T_{\rho}(P_X(z_k-\rho F(z_k)) )\right], \quad \tau >0
\end{equation}
where $P\in \nbR^{n\times n}$ is a PD matrix and  $T_\rho=I-\rho F$
The hyperplane projection method has been introduced by \citep{Konnov1993}. The convergence has been proved under the assumptions that $F$ is  a continuous pseudo-monotone mapping. The method is described in Algorithm~\ref{Algo:HPA}.
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf F,X$
      \REQUIRE $\sf z_0 \in X, \tau >0, \sigma \in (0,1)$
      \ENSURE  $\sf z$ solution of $\sf \mathrm{VI}(X,F)$ with $F$ a continuous pseudo-monotone mapping
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE $\sf y_k \leftarrow  P_X(z_k-\tau F(z_k))$
      \STATE (Armijo line--search) Find the smallest integer, $\sf i\in \nbN$ such that 
      \begin{equation}
        \label{eq:HPA1}
        \sf  F(2^{-i}y_k+(1-2^{-i})z_k)^T (z_k-y_k) \geq \Frac \sigma \tau \|z_k-y_k\|^2
      \end{equation}
      \STATE $\sf i_k \leftarrow i$
      \STATE $\sf x^k \leftarrow 2^{-i_k}y_k+(1-2^{-i_k})z_k$
      \STATE $\sf w_k \leftarrow  z_k - \Frac{F(x_k)^T(z_k-x_k)}{\|F(x_k)\|^2}\,F(x_k)$
      \STATE $\sf z_{k+1}  \leftarrow P_{X}(w_k)$
      \STATE $\sf k \leftarrow k+1$
      \STATE Evaluate $\sf error$.
      \ENDWHILE
    }
  \end{algorithmic}
  \caption{Hyperplane projection method \citep{Konnov1993}}  \label{Algo:HPA}
\end{algorithm}

\begin{table}
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    { Algorithm}
    & Description 
    & parameters\\
    \hline
    \sf EG-VI-1 
    & Algorithm~\ref{Algo:ExtraGradient-vi} with the formulation (\ref{eq:vi-I})
    & $\rho$\\ 
    \hline
    \sf EG-VI-2 
    & Algorithm~\ref{Algo:ExtraGradient-vi} with the formulation (\ref{eq:vi-II})
    & $\rho$\\ 
    \hline
    \sf HPA-VI-1 
    & Algorithm~\ref{Algo:HPA} with the formulation (\ref{eq:vi-I})
    & $\sigma,\tau$\\
    \hline
    \sf HPA-VI-2 
    & Algorithm~\ref{Algo:HPA} with the formulation (\ref{eq:vi-II})
    & $\sigma,\tau$\\ 
    \hline
  \end{tabular}
  \caption{Naming convention for the algorithms}
  \label{tab:Projection-algos}
\end{table}


\subsection{Splitting techniques and proximal point algorithm}

Splitting techniques are standard techniques to solve $\mathrm{VI}(F,X)$ when the function $F$ is affine, that is $F=Mz+q$. Usually, a block splitting of the matrix $M$ is performed and a Projected Successive Over Relaxation(PSOR) is used to solve the VI. Due to the particular structure of the cone $K$, that can be considered is a splitting contact by contact or by group of several contacts. The sub-problems that are generated can be then solved by a dedicated method for the VI that have been presented in the previous sections. In the same way, the proximal point algorithm can also be used which amounts to solving the original VI $\mathrm{VI}(F,X)$ by solving a sequence of $\mathrm{VI}(F_{c,x_k},X)$  problems such that $  F_{c,x}(z) = z - x + c F(z) , \quad c > 0$ and $\lim_ {k \rightarrow +\infty } \|x_k-z\| =0$.
This intermediate VI can be solved with any solvers presented in the previous sections. 

 Rather than entering into deeper details, we prefer to refer to Sect~\ref{Sec:SplittingTechniquesAndProx} for a more general family of solvers based on splitting techniques and proximal point algorithms 


\clearpage
\section{Newton based methods}


\subsection{Nonsmooth Newton's method}

In Section~\ref{Sec:NonsmoothEquations}, several formulations of the frictional contact problems by means of nonsmooth equations  have been presented. These nonsmooth equations call for the use of nonsmooth Newton's methods. If we consider the general equation~(\ref{eq:ne-1}) with $z = [u,v,r]^T$ we get
\begin{equation}
  \label{eq:NSN1}
  G(z)=0
\end{equation}
which can be solved by the following newton iteration
\begin{equation}
  \label{eq:NSN2}
  z_{k+1}  =  z_k -  J^{-1}(z_k) (G(z_k)).
\end{equation}
If the mapping $G$ is smooth enough, the matrix $J$ is the Jacobian matrix of $G$ with respect to $z$, that is $J(z) = \nabla^T_z G(z)$. When the $G$ is nonsmooth but Lipschitz,  the choice of a substitute to the Jacobian matrix can be given by an element of the Clarke sub--differential at $z$ denoted by $ \Phi(z) \in \partial G(z)$. Let us assume for a moment that the $\Phi$ is nonsingular, a nonsmooth Newton method can be written as 
\begin{equation}
  \label{eq:NSN3}
  z_{k+1}  =  z_k -  \Phi^{-1}(z_k) (G(z_k)).
\end{equation}
The resulting nonsmooth Newton method is detailed in Algorithm~\ref{Algo:NSN}.
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf G $ Data of Problem~(\ref{eq:NSN1})
      \REQUIRE $\sf D$ a symmetric positive definite matrix
      \REQUIRE $\sf z_0$ initial values and $\sf tol >0$ a tolerance
      \ENSURE  $\sf z$ solution of Problem~(\ref{eq:NSN1})
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE compute (choose) $\sf \Phi(z_k) \in \partial G(z_k)$
      \STATE $\sf z_{k+1} \leftarrow   z_k -  \Phi^{-1}(z_k) (G(z_k))$
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$
      \ENDWHILE
      \STATE $\sf z \leftarrow z_{k}$ 
    }
  \end{algorithmic}
  \caption{Nonsmooth Newton method for (\ref{eq:NSN1})}  \label{Algo:NSN}
\end{algorithm}

\begin{ndrva}
  General results of local convergence ?
\end{ndrva}

\subsubsection{Nonsmooth equations based Normal maps }
Let us consider the natural map $F_{\vitwo}^\nat$ in~(\ref{eq:natural-II}) that enables to write Problem~\ref{prob:II} in a nonsmooth equation. Let us introduce the following notation for an element of the sub--differential
\begin{equation}
  \label{eq:Phi-natural-typeII}
  \Phi(u, r)  = \left[
  \begin{array}{cc}
    \rho  I  &   - \rho W \\
     \Phi_{r u}(u,r) &   \Phi_{r r}(u,r)  
  \end{array}\right] \in \partial F_{\vitwo}^\nat(u,r)
\end{equation}
where $ \Phi_{x y}(u,r) \in \partial_{x}[F_{\vitwo}^\nat]_{y}(u,r)$. For one contact, a possible computation of the remaining parts in $\Phi(u, r)$  is given by
\begin{equation}
  \label{eq:Phi-natural-typeII-rr}
   \Phi_{r u}(u,r)  = 
  \left\{
    \begin{array}{lcl}
      0  & \text { if } & r- \rho(u+g(u)) \in K    \\ \\
      I - \partial_{r}[P_K(r-\rho(u+g(u)))]  & \text { if } &r- \rho(u+g(u))   \notin K
    \end{array}\right.
\end{equation}

\begin{equation}
  \label{eq:Phi-natural-typeII-ru}
  \Phi_{r u}(u,r) = 
  \left\{
    \begin{array}{lcl}
      \rho \left(I +
      \left[\begin{array}{ccc}
          0 & 0 & 0 \\
          \Frac{u_\t}{\|u_\t\|} & 0 & 0 \\
        \end{array}\right]
      \right)  & \text { if } &
      \begin{cases}
        r- \rho(u+g(u)) \in K \\ u_\t \neq 0
      \end{cases}
      \\ \\
      \rho\left(I +  \left[\begin{array}{ccc}
          0 & 0 & 0 \\
          s & 0 & 0 \\
        \end{array}\right]\right), s \in \RR^2 , \|s\|=1  & \text { if } &
    \begin{cases}
      r- \rho(u+g(u)) \in K \\ u_\t = 0
    \end{cases}
    \\ \\
      I        +\rho \left(I +  \left[\begin{array}{ccc}
          0 & 0 & 0 \\
          \Frac{u_\t}{\|u_\t\|} & 0 & 0 \\
        \end{array}\right]\right)    \partial_{u}[P_K(r-\rho(u+g(u)))] & \text { if } &r- \rho(u+g(u))   \notin K
    \end{array}\right.
\end{equation}
The computation of an element of $\partial P_K$ is given in Appendix~\ref{Sec:Ann:ConvexAnalysis}. Since $\Phi_{u u}(u,r) = I $, a reduction of the system is performed in practise and Algorithm~\ref{Algo:NSN} is applied or $z =r$ with
\begin{equation}
  \label{eq:phiphi}
  \begin{cases}
    G(z) = [F_{\vitwo}^\nat]_{r}(Wr+q,r) \\
    \Phi(z) = \Phi_{rr}(r,Wr+q) + \Phi_{ru}(r,Wr+q) W
  \end{cases}
\end{equation}
\begin{ndrva}
  to be checked carefully
\end{ndrva}

Let us consider the natural map $F_{\vione}^\nat$ in~(\ref{eq:natural-I}) that enables to write Problem~\ref{prob:I} in a nonsmooth equation. Let us introduce the following notation for an element of the sub--differential with a obvious simplification
\begin{equation}
  \label{eq:Phi-natural-typeI}
  \Phi(v, u, r)  = \left[
  \begin{array}{ccc}
   \rho M &  0 &   - \rho H  \\
   -\rho H^\top &  \rho I &   0  \\
   0  &   \Phi_{r u}(v,u,r) &   \Phi_{r r}(v,u,r)  
 \end{array}\right] \in \partial F_{\vitwo}^\nat(u,r)
\end{equation}
where $ \Phi_{x y}(v,u,r) \in \partial_{x}[F_{\vione}^\nat]_{y}(v,u,r)$. A possible computation of  $\Phi_{r u}(v,u,r)$ and $\Phi_{r r}(v,u,r) $ is directly given by~(\ref{eq:Phi-natural-typeII-ru}) and (\ref{eq:Phi-natural-typeII-rr}). In this case, the variable $u$ can be also substituted.

Similar computations can be found in~\cite{Joli.Feng2008} where a Newton method based on the formulation~(\ref{eq:natural-II}) is used contact by contact in a Gauss--Seidel loop.

\begin{ndrva}
Idea :  Iteration can also be interesting to state a Newton method on the normal map since the evaluation of $F$ is only made on $K$ and not outside. Perhaps, we can control the regularity of the subgradients of $F(P_X(z))$
\end{ndrva}
\subsubsection{Alart and Curnier function}
\label{Sec:NSN-AC}
Let us consider now the Alart--Curnier function $F_{\actwo}(u,r)$ in~(\ref{eq:AC-II}) for Problem~\ref{prob:II}. As in the previous section, the following notation is introduced for an element of the sub--differential
\begin{equation}
  \label{eq:Phi-AC-II}
  \Phi(u, r)  = \left[
  \begin{array}{cccc}
    \multicolumn{2}{c}{\rho  I}  &  \multicolumn{2}{c}{- \rho W} \\ \\
     \Phi_{r_\n u_\n}(u,r) & 0\quad\quad&   \Phi_{r_\n r_\n}(u,r) & 0 \\ 
     \Phi_{r_\t u_\n}(u,r) & \Phi_{r_\t u_\t}(u,r) \quad\quad&   \Phi_{r_\t r_\n}(u,r) & \Phi_{r_\t r_\t}(u,r) 
  \end{array}\right] \in \partial F_{\actwo}(u,r)
\end{equation}
For one contact, a possible computation of the remaining parts in $\Phi(u, r)$  is given by

\begin{equation}
  \label{eq:Phi-AC-II-rnun}
   \Phi_{r_\n u_\n}(u,r) =  \left\{
   \begin{array}{ll}
     \rho_{\n} & \text{ if }  r_{\n} - \rho_{\n} u_{\n} > 0 \\
     0  & \text{ otherwise } 
    \end{array}\right.
\end{equation}
\begin{equation}
  \label{eq:Phi-AC-II-rnrn}
  \Phi_{r_\n r_\n}(u,r) = 
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  r_{\n} - \rho_{\n} u_{\n} > 0 \\
      1  & \text{ otherwise } 
    \end{array}\right. 
\end{equation}

\begin{equation}
  \label{eq:Phi-AC-II-rtun}
  \Phi_{r_\t u_\n}(u,r) =   
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
      0 & \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n\leq 0
      \end{cases} \\
     \mu \rho_{\n}  \Frac{r_{\t} - \rho_{\t} u_{\t} }{ \| r_{\t} - \rho_{\t} u_{\t}\| }   &  \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n > 0
      \end{cases} \\
    \end{array}\right. 
\end{equation}
\begin{equation}
  \label{eq:Phi-AC-II-rtut}
  \Phi_{r_\t u_\t}(u,r) =   
  \left\{
    \begin{array}{ll}
      \rho_{\t} & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
     \mu\rho_{\t}(r_{\n} - \rho_{\n} u_\n )_+ \Gamma(r_{\t} - \rho_{\t} u_{\t})  
      &  \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n > 0
      \end{cases} \\
    \end{array}\right. 
\end{equation}
\begin{equation}
  \label{eq:Phi-AC-II-rtrn}
  \Phi_{r_\t r_\n}(u,r) =   
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
      0 & \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n\leq 0
      \end{cases} \\
       -\mu  \Frac{r_{\t} - \rho_{\t} u_{\t} }{ \| r_{\t} - \rho_{\t} u_{\t}\| }  &  \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n > 0
      \end{cases} \\
    \end{array}\right. 
\end{equation}
\begin{equation}
  \label{eq:Phi-AC-II-rtrt}
  \Phi_{r_\t r_\t}(u,r) =   
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
      I_2-\mu(r_{\n} - \rho_{\n}u_\n )_+ \Gamma(r_{\t} - \rho_{\t} u_{\t})    &  \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu \max (0 ,r_{\n} - \rho_{\n} u_\n )  \\
        r_{\n} - \rho_{\n} u_n > 0
      \end{cases} \\
    \end{array}\right. 
\end{equation}
with 
 the function $\Gamma(\cdot)$  defined by
\begin{equation}
  \label{eq:AC-L12}
  \Gamma(x) = \Frac{I_{2\times 2}}{\|x\|} - \Frac{x\,x^\top}{\|x\|^3}
\end{equation}

If the variant~(\ref{eq:CKPS-1}) is chosen, the computation of $\Phi_{r_\t \bullet}$  simplify in
\begin{equation}
  \label{eq:Phi-CKPS-II-rtun}
  \Phi_{r_\t u_\n}(u,r) =   0
\end{equation}
\begin{equation}
  \label{eq:Phi-CKPS-II-rtut}
  \Phi_{r_\t u_\t}(u,r) =   
  \left\{
    \begin{array}{ll}
      \rho_{\t} & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu r_\n  \\
      -\mu\rho_\t r_{n,+} \Gamma(r_\t-\rho_\t u_\t) & \text{ if }
      \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu r_\n  \\
    \end{array}\right. 
\end{equation}
\begin{equation}
  \label{eq:Phi-CKPS-II-rtrn}
  \Phi_{r_\t r_\n}(u,r) =   
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu r_{\n}  \\
      0 & \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu r_n  \\
        r_{\n} \leq 0
      \end{cases} \\
       -\mu  \Frac{r_{\t} - \rho_{\t} u_{\t} }{ \| r_{\t} - \rho_{\t} u_{\t}\| }  &  \text{ if }
      \begin{cases}
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu r_n  \\
        r_{\n}  > 0
      \end{cases} \\
    \end{array}\right. 
\end{equation}

\begin{equation}
  \label{eq:Phi-CKPS-II-rtrt}
  \Phi_{r_\t r_\t}(u,r) =   
  \left\{
    \begin{array}{ll}
      0 & \text{ if }  \|r_{\t}  - \rho_{\t} u_{\t}\| \leq  \mu r_{\n}\\
      I_2-\mu r_{\n,+} \Gamma(r_{\t} - \rho_{\t} u_{\t})    &  \text{ if }
        \|r_{\t}  - \rho_{\t} u_{\t}\| >  \mu r_{\n}  \\
    \end{array}\right. 
\end{equation}
\begin{ndrva}
  Is there a difference with the computation of Florent in his thesis ?
\end{ndrva}


For Problem~\ref{prob:I}, the same structure of the sub-gradient as in~(\ref{eq:Phi-natural-typeI}) can be introduced and the previous computation of $\Phi_{r\bullet}$ remains valid.

\begin{remark}
  As in the previous section, the particular structure of the subgradient in ~(\ref{eq:Phi-AC-II}) calls for a substitution of the variable $u$. This is done in the numerical practice.
\end{remark}

\subsubsection{SOCCP function}

\begin{ndrva}
  This is perhaps  too much
\end{ndrva}

\subsection{Damped Newton and Line-search procedures}


\begin{itemize}
\item   (GP, Armijo, Non-monotone watch dogs)
\end{itemize}


\subsection{Nomenclature}
\begin{itemize}
\item Alart Curnier
\item Christensen et al.\cite{Christensen.Klarbring.ea1998}
\item Newton sur De Saxce cf P. Joly
\item and others ?
\end{itemize}
\begin{table}[htbp]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    { Algorithm}
    & Description 
    & parameters\\
    \hline
    \sf NSN-NA-1 
    &  Algorithm~\ref{Algo:NSN} with natural map for Problem~\ref{prob:I}
    & \\
   \hline
    \sf NSN-NA-2 
    &  Algorithm~\ref{Algo:NSN} with natural map for Problem~\ref{prob:II}
    & \\
    \hline
     \sf NSN-AC-1 
    &  Algorithm~\ref{Algo:NSN} with Alart-Curnier for Problem~\ref{prob:I}  
    & \\
   \hline
    \sf NSN-AC-2 
    &  Algorithm~\ref{Algo:NSN} with Alart-Curnier for Problem~\ref{prob:II}
    & \\
    \hline
     \sf NSN-CKPS-1 
    &  Algorithm~\ref{Algo:NSN} with CKPS for Problem~\ref{prob:I}
    & \\
   \hline
    \sf NSN-CKPS-2 
    &  Algorithm~\ref{Algo:NSN} with CKPS for Problem~\ref{prob:II}
    & \\
   \hline
    \sf NSN-NA-GP-1 
    &  Algorithm~\ref{Algo:NSN} with natural map for Problem~\ref{prob:I} and GP line search
    & \\
  \hline
     \sf \ldots 
    &  \ldots
    & \\
  \hline
     
   \end{tabular}
  \caption{Naming convention for the algorithms based on nonsmooth Newton method}
  \label{tab:NSN-algos}
\end{table}

\clearpage
\section{Splitting techniques and proximal point algorithm}
\label{Sec:SplittingTechniquesAndProx}

\subsection{Splitting techniques}
\label{Sec:SplittingTechniques}
 The particular structure of the cone $K$ as a product of second-order cone in $\RR^3$ suggests a special splitting of the problem for each contact labelled by $\alpha \in 1\ldots n_c$. 

For Problem~\ref{prob:II}, the relation
\begin{equation}
  \label{eq:delassus-bis}
  u = W r+q
\end{equation}
is split along each contact as follows
\begin{equation}
  \label{eq:delassus-ter}
  u^\alpha = W^{\alpha\alpha} r^\alpha + \sum_{\beta\neq \alpha}W^{\alpha\beta} r^\beta +  q^\alpha, \text{ for all } \alpha \in 1\ldots n_c,
\end{equation}
where the matrices $W^{\alpha\beta}, \alpha \in 1\ldots n_c, \beta \in 1\ldots n_c $ are easily identified from~(\ref{eq:delassus-bis}).
 
From~(\ref{eq:delassus-ter}), a projected Gauss--Seidel (PGS) method can be designed by using the following rule at the iterate $k$
\begin{equation}
  \label{eq:pgs-1}
  u^{\alpha}_{k+1} = W^{\alpha\alpha} r^{\alpha}_{k+1} + \sum_{\beta < \alpha}W^{\alpha\beta} r^{\beta}_{k+1} + \sum_{\beta > \alpha}W^{\alpha\beta} r^{\beta}_{k} +  q^\alpha, \text{ for all } \alpha \in 1\ldots n_c,
\end{equation}
and more generally a Projected Successive Over Relaxation(PSOR) is obtained by introducing a relaxation parameter $\omega>0$ such that
\begin{equation}
  \label{eq:psor-1}
  u^{\alpha}_{k+1} = \frac 1 \omega W^{\alpha\alpha} r^{\alpha}_{k+1} 
  - \frac 1 \omega W^{\alpha\alpha} r^{\alpha}_{k} +
  \sum_{\beta < \alpha}W^{\alpha\beta} r^{\beta}_{k+1} + \sum_{\beta \geq \alpha}W^{\alpha\beta} r^{\beta}_{k} +  q^\alpha, \text{ for all } \alpha \in 1\ldots n_c.
\end{equation}
By denoting 
\begin{equation}
  \label{eq:psor-2}
  \begin{cases}
    \Bar W^{\alpha\alpha} = \frac 1 \omega W^{\alpha\alpha} \\
    \bar q^{\alpha}_{k+1} = - \frac 1 \omega W^{\alpha\alpha} r^{\alpha}_{k}
    + \sum_{\beta < \alpha}W^{\alpha\beta} r^{\beta}_{k+1} + \sum_{\beta
      \geq \alpha}W^{\alpha\beta} r^{\beta}_{k} + q^\alpha
  \end{cases}
, \text{ for all } \alpha \in 1\ldots n_c,
\end{equation}
we have to solve at each iteration the following problem on each contact $\alpha$
\begin{equation}\label{eq:psor-3}
  \begin{cases}
    u^{\alpha}_{k+1} =  \Bar W^{\alpha\alpha}  r^{\alpha}_{k+1} + \bar q^{\alpha}_{k+1}, \\
    \hat u^{\alpha}_{k+1} =u^{\alpha}_{k+1} + g(u^{\alpha}_{k+1}), \\[2mm]
    K^{\alpha,\star} \ni {\hat u^{\alpha}_{k+1}} \perp r^{\alpha}_{k+1} \in K^\alpha.
  \end{cases}
\end{equation}
The problem on each contact~(\ref{eq:psor-3}) has exactly the same structure as Problem~\ref{prob:II}, but only concerns one contact. It can be solved analytically of by invoking one of the algorithms presented in this paper a local solver for the sub-problems. The PSOR algorithm is summarized in Algorithm~\ref{Algo:PSOR-II}.

\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $ 
      \REQUIRE $\sf W,q,\mu $ Date of Problem~\ref{prob:II} 
      \REQUIRE $\sf r_0$ initial values and $\sf tol >0$ a tolerance
      \REQUIRE $\sf \omega$ a relaxation parameter 
      \ENSURE  $\sf r,u$ solution of Problem~\ref{prob:II}
      \WHILE {$\sf error > tol$} 
      \FOR {$\sf \alpha = 1 \ldots n_c$}
      \STATE $\sf \bar W^{\alpha\alpha}_{k+1} \leftarrow  \frac 1 \omega W^{\alpha\alpha}$
      \STATE $\sf   \bar q^{\alpha}_{k+1} \leftarrow - \frac 1 \omega W^{\alpha\alpha} r^{\alpha}_{k}
      + \sum_{\beta < \alpha}W^{\alpha\beta} r^{\beta}_{k+1} + \sum_{\beta
        \geq \alpha}W^{\alpha\beta} r^{\beta}_{k} + q^\alpha $
      \STATE Solve the single contact problem $\sf \mathrm{FC/II}(\bar W^{\alpha\alpha},\bar q^{\alpha}_{k+1},\mu)$
      \ENDFOR
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$ 
      \ENDWHILE
      \STATE $\sf r \leftarrow r_{k}$ 
      \STATE $\sf u \leftarrow u_{k}$ 
    }
  \end{algorithmic}
  \caption{PSOR algorithm for Problem~\ref{prob:II}}  \label{Algo:PSOR-II}
\end{algorithm}

In~\cite{Jourdan.Alart.ea98}, this method is developed in the Gauss-Seidel configuration ($\omega=1$) with a local Newton solver based on the Alart--Curnier formulation. If the local solver is only one iteration of the VI solver based on projection, we get a standard splitting techniques for VI. In Table~\ref{tab:PSOR-algos}, the methods based on PSOR used in the comparison are summarized.
\begin{table}
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    { Algorithm}
    & Description 
    & parameters\\
    \hline
    \sf PSOR-AC-1 
    &  Algorithm~\ref{Algo:PSOR-II} with the local solver ??
    & $\omega,tol$\\
    \hline
    \sf PSOR-VI-1 
    &  Algorithm~\ref{Algo:PSOR-II} with one iteration of {\sf FP-VI-2} for the local solver. 
    & $\omega,tol$ and  $D$ or $\rho$\\
   \hline    
    \sf PSOR-VI-2 
    &  Algorithm~\ref{Algo:PSOR-II} with {\sf FP-VI-2}  for the local solver up to $tol_2$ tolerance. 
    & $\omega,tol,tol_2$ and  $D$ or $\rho$\\
   \hline
   \ldots \\
   \hline
  \end{tabular}
  \caption{Naming convention for the algorithms based on PSOR}
  \label{tab:PSOR-algos}
\end{table}

\begin{ndrva}
 Do not forget the overrelaxation and some heuristic to adapt convergence (line search).
  
 Aitken acceleration. Lebon, Raous et al.

 Problem~\ref{prob:I} ??
\end{ndrva}

\subsection{Proximal points techniques}

Without entering into theoretical details, the key idea of proximal points algorithms it to replace the original $\mathrm{VI}(F,X)$ by a sequence of $\mathrm{VI}(F_{\rho,x_k},X)$  problems such that 
\begin{equation}
  \label{eq:prox-algo-1}
  F_{\rho,x}(z) = z - x _k+ \rho F(z) , \quad \rho > 0 
\end{equation}
with the property that $\lim_{ k \rightarrow +\infty} \| x_k -z \| = 0$. This is usually performed by defining a sequence $x_k$ such that
\begin{equation}
  \label{eq:prox-algo-2}
  x_{k+1} = (1-\omega) x_{k} + \omega z_{k+1}
\end{equation}
where $\omega$ is a relaxation parameter and $z_{k+1}$ the solution of $\mathrm{VI}(F_{\rho,x_k},X)$. The algorithm is described in algorithm~\ref{Algo:PPA-vi}
\begin{algorithm}
  \begin{algorithmic}
    {\sf
      \STATE $ $
      \REQUIRE $\sf F,X$ Data of Problem~\ref{eq:vi}
      \REQUIRE $\sf \omega, \rho$ relaxation parameters
      \REQUIRE $\sf x_0$ initial values and $\sf tol >0, tol_2$ tolerances
      \ENSURE  $\sf z$ solution of Problem~\ref{eq:vi}
      \STATE   $\sf k \leftarrow 0$ 
      \WHILE {$\sf error > tol$} 
      \STATE Solve $\mathrm{VI}(F_{\rho,x_k},X)$ for $\sf z_{k+1}$ with tolerance $tol_2$
      \STATE $\sf x_{k+1} \leftarrow (1-\omega) x_{k} + \omega z_{k+1}$
      \STATE Evaluate $\sf error$.
      \STATE $\sf k \leftarrow k+1$
      \ENDWHILE
      \STATE $\sf z \leftarrow z_{k}$ 
    }
  \end{algorithmic}
  \caption{Proximal point algorithm for the VI~(\ref{eq:vi})}  \label{Algo:PPA-vi}
\end{algorithm}

For solving the sub-problem $\mathrm{VI}(F_{\rho,x_k},X)$, any of the previous presented algorithms can be used. In this sense, the proximal point algorithm defined a general family of algorithm. The main interest of the proximal point algorithm is the regularization it introduces in the  definition (\ref{eq:prox-algo-1}). For instance, let consider an affine VI, that is defined with $F(z)=M z+q$. The function is the sub-problem is given by
\begin{equation}
  \label{eq:prox-algo-4}
   F_{\rho,x}(z) = (I + \rho M) z - x_k + \rho q , \quad \rho > 0 .
\end{equation}
We can easily that for sufficiently small $\rho$, we get a monotone affine VI and even better an strongly monotone VI. For Problem~\ref{prob:II} with $F_{\vitwo}$, the proximal point algorithm yields
\begin{equation}
  \label{eq:prox-algo-5}
   F_{\vitwo,\rho,x}(r) = (I + \rho W) r - x_k + \rho( q + g(W r +q )) , \quad \rho > 0 .
\end{equation}
and it should not be difficult to prove that with a small $\rho$ parameter that we get a monotone VI.  Naturally, there is a price to pay, smaller the parameter $\rho$ is, easier the VI problem is, but the resulting evaluation of $z_k$ is far from the solution.

\begin{ndrva}
  \begin{itemize}
\item     Prove it !
\item Is there a analogy with choosing a small $r$ ?
  \end{itemize}

\end{ndrva}

The use of proximal point algorithm can be very interesting when the solving of the problem suffers for the lack of regularity of the operator. For instance, the Newton methods are in trouble when the Jacobian is not invertible. Thanks to the proximal point algorithm, we can retrieve invertible Jacobian.

\begin{table}
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    { Algorithm}
    & Description 
    & parameters\\
    \hline
    \sf PPA-AC-1 
    &  Algorithm~\ref{Algo:PPA-vi} with Alart-Curnier 
    & $\omega,\rho$\\
   \hline
  \end{tabular}
  \caption{Naming convention for the algorithms based on PPA}
  \label{tab:PPA-algos}
\end{table}
\section{Optimization based methods}
\label{Sec:OptimisationBasedMethods}

\subsection{Fixed point on the Friction threshold. Haslinger Approach}

methods of successive approximation successive approximations is a natural tool for the numerical realization [3]. Each iterative step is represented by an auxiliary contact problem with given friction described by a variational inequality of the second kind.

kucera Dostal Haslinger ICNAAM 2004  + panagiotopoulos. Un coup N un coup T


\subsection{ACLM approach}

\subsection{SOCCP approach}


\cite{Hayashi.ea2005} with Kanno's formulation ?

\section{Measuring errors}
\label{Sec:MeasuringErrors}

\section{Numerical comparisons}


\subsection{Benchmarks presentation}

\subsection{Comparison of methods by family}


\subsection{Robustness}

\subsection{Convergence rate}

\subsection{CPU and memory efforts}




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "rr"
%%% End: 
